{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"../datasets/attribute_set/train_val_data_fine_grained_all.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16000 entries, 0 to 1999\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paths            16000 non-null  object\n",
      " 1   floral           16000 non-null  int64 \n",
      " 2   graphic          16000 non-null  int64 \n",
      " 3   striped          16000 non-null  int64 \n",
      " 4   embroidered      16000 non-null  int64 \n",
      " 5   pleated          16000 non-null  int64 \n",
      " 6   solid            16000 non-null  int64 \n",
      " 7   lattice          16000 non-null  int64 \n",
      " 8   long_sleeve      16000 non-null  int64 \n",
      " 9   short_sleeve     16000 non-null  int64 \n",
      " 10  sleeveless       16000 non-null  int64 \n",
      " 11  maxi_length      16000 non-null  int64 \n",
      " 12  mini_length      16000 non-null  int64 \n",
      " 13  no_dress         16000 non-null  int64 \n",
      " 14  crew_neckline    16000 non-null  int64 \n",
      " 15  v_neckline       16000 non-null  int64 \n",
      " 16  square_neckline  16000 non-null  int64 \n",
      " 17  no_neckline      16000 non-null  int64 \n",
      " 18  denim            16000 non-null  int64 \n",
      " 19  chiffon          16000 non-null  int64 \n",
      " 20  cotton           16000 non-null  int64 \n",
      " 21  leather          16000 non-null  int64 \n",
      " 22  faux             16000 non-null  int64 \n",
      " 23  knit             16000 non-null  int64 \n",
      " 24  tight            16000 non-null  int64 \n",
      " 25  loose            16000 non-null  int64 \n",
      " 26  conventional     16000 non-null  int64 \n",
      "dtypes: int64(26), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>floral</th>\n",
       "      <th>graphic</th>\n",
       "      <th>striped</th>\n",
       "      <th>embroidered</th>\n",
       "      <th>pleated</th>\n",
       "      <th>solid</th>\n",
       "      <th>lattice</th>\n",
       "      <th>long_sleeve</th>\n",
       "      <th>short_sleeve</th>\n",
       "      <th>...</th>\n",
       "      <th>no_neckline</th>\n",
       "      <th>denim</th>\n",
       "      <th>chiffon</th>\n",
       "      <th>cotton</th>\n",
       "      <th>leather</th>\n",
       "      <th>faux</th>\n",
       "      <th>knit</th>\n",
       "      <th>tight</th>\n",
       "      <th>loose</th>\n",
       "      <th>conventional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Sweet_Crochet_B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Classic_Pencil_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Strapless_Diamo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Mid-Rise_-_Acid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Zippered_Single...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paths  floral  graphic  \\\n",
       "0  ../datasets/big_ds/img-001/img/Sweet_Crochet_B...       0        0   \n",
       "1  ../datasets/big_ds/img-001/img/Classic_Pencil_...       0        0   \n",
       "2  ../datasets/big_ds/img-001/img/Strapless_Diamo...       0        1   \n",
       "3  ../datasets/big_ds/img-001/img/Mid-Rise_-_Acid...       0        0   \n",
       "4  ../datasets/big_ds/img-001/img/Zippered_Single...       0        0   \n",
       "\n",
       "   striped  embroidered  pleated  solid  lattice  long_sleeve  short_sleeve  \\\n",
       "0        0            1        0      0        0            0             0   \n",
       "1        0            0        0      1        0            0             0   \n",
       "2        0            0        0      0        0            0             0   \n",
       "3        0            0        0      1        0            0             0   \n",
       "4        0            0        0      1        0            1             0   \n",
       "\n",
       "   ...  no_neckline  denim  chiffon  cotton  leather  faux  knit  tight  \\\n",
       "0  ...            1      0        1       0        0     0     0      0   \n",
       "1  ...            1      0        0       1        0     0     0      1   \n",
       "2  ...            1      0        0       1        0     0     0      0   \n",
       "3  ...            1      1        0       0        0     0     0      1   \n",
       "4  ...            0      0        0       1        0     0     0      0   \n",
       "\n",
       "   loose  conventional  \n",
       "0      0             1  \n",
       "1      0             0  \n",
       "2      0             1  \n",
       "3      0             0  \n",
       "4      0             1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 14000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Pipeline by using tf.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['../datasets/big_ds/img-001/img/Sweet_Crochet_Blouse/img_00000070.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Classic_Pencil_Skirt/img_00000010.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Strapless_Diamond_Print_Dress/img_00000038.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Mid-Rise_-_Acid_Wash_Skinny_Jeans/img_00000010.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Zippered_Single-Button_Blazer/img_00000078.jpg'],\n",
       "       dtype=object),\n",
       " 16000)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = data.paths.to_numpy()  \n",
    "fnames[:5], len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "ds_size = data.shape[0] \n",
    "number_of_selected_samples = 2000 \n",
    "\n",
    "# filelist_ds = tf.data.Dataset.from_tensor_slices(fnames[:number_of_selected_samples]) \n",
    "filelist_ds = tf.data.Dataset.from_tensor_slices(fnames) \n",
    "\n",
    "\n",
    "filelist_ds.cardinality().numpy() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    \"\"\"\n",
    "        file_path: the file path for the image that you want to select\n",
    "    \"\"\"\n",
    "    labels = data.loc[data.paths == file_path].to_numpy().squeeze()[1:].astype(\"int64\")\n",
    "    return tf.convert_to_tensor(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(26,), dtype=int64, numpy=\n",
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1], dtype=int64)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label(fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and scale the images so that we can save time in training  \n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 \n",
    "def decode_img(img):\n",
    "    \"\"\"\n",
    "        img: img is the image \n",
    "    \"\"\" \n",
    "    #color images \n",
    "    img = tf.image.decode_jpeg(img, channels=3) \n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)  #convert unit8 tensor to floats in the [0,1] range\n",
    "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \n",
    "    # img = img / tf.constant(256, dtype=tf.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images_labels(file_path: tf.Tensor): \n",
    "    label = get_label(file_path) \n",
    "    img = tf.io.read_file(file_path) \n",
    "    img = decode_img(img) \n",
    "    return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_images_labels(fnames[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = filelist_ds.take(TRAIN_SIZE) \n",
    "ds_test = filelist_ds.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process All the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(lambda x: \n",
    "                        tf.py_function(func=combine_images_labels, \n",
    "                                       inp=[x], # input of the function \n",
    "                                       Tout=(tf.float32,tf.int64)),  # return type \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE, # parallelizing data extraction \n",
    "                        deterministic=False \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test= ds_test.map(lambda x: tf.py_function(func=combine_images_labels,\n",
    "          inp=[x], Tout=(tf.float32,tf.int64)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Pipeline \n",
    "\n",
    "- **batch**(): Combines consecutive elements of this dataset into batches.\n",
    "- **cache**(): Caches the elements in this dataset. he first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.Subsequent iterations will use the cached data.\n",
    "- **prefetch**(): Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to *prefetch*. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_batched = ds_train.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) \n",
    "ds_test_batched = ds_test.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_batched.cardinality().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes = len(data.columns) - 1  \n",
    "nr_of_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1:Fine Grained VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "base_model = keras.applications.VGG16(\n",
    "    weights=\"imagenet\", # load weights pre-trained on ImageNet. \n",
    "    input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), # VGG16 expects min 32 x 32 \n",
    "    include_top = False # do not include output layer of the image net vgg \n",
    ")\n",
    "base_model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
    "x = tf.cast(inputs, tf.float32)\n",
    "x = tf.keras.applications.vgg16.preprocess_input(x)   \n",
    "x = base_model(x) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_1 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) \n",
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=True,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1504\\2976827453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history_model_1 = model_1.fit(ds_train_batched, \n\u001b[0m\u001b[0;32m      2\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                             \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mds_test_batched\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             )\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;31m# no_variable_creation function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m       _, _, filtered_flat_args = (\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\melih.kacaman\\Anaconda3\\envs\\env_similar-products\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_model_1 = model_1.fit(ds_train_batched, \n",
    "                            epochs=10,\n",
    "                            validation_data =ds_test_batched,\n",
    "                            callbacks=[early_stopping, checkpoint_callback]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1.save(\"../trained_models/model_1_fine_grained_vgg.h5\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Efficient Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "efficient_net.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3)) \n",
    "x = efficient_net(inputs) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_2 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                33306     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,082,877\n",
      "Trainable params: 33,306\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) \n",
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained_model3/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=True,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2891 - binary_accuracy: 0.8815\n",
      "Epoch 1: val_loss improved from inf to 0.24964, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 294s 650ms/step - loss: 0.2891 - binary_accuracy: 0.8815 - val_loss: 0.2496 - val_binary_accuracy: 0.8974\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2440 - binary_accuracy: 0.8994\n",
      "Epoch 2: val_loss improved from 0.24964 to 0.23676, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 317s 721ms/step - loss: 0.2440 - binary_accuracy: 0.8994 - val_loss: 0.2368 - val_binary_accuracy: 0.9028\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2332 - binary_accuracy: 0.9036\n",
      "Epoch 3: val_loss improved from 0.23676 to 0.23082, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 316s 718ms/step - loss: 0.2332 - binary_accuracy: 0.9036 - val_loss: 0.2308 - val_binary_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2265 - binary_accuracy: 0.9062\n",
      "Epoch 4: val_loss improved from 0.23082 to 0.22766, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 313s 710ms/step - loss: 0.2265 - binary_accuracy: 0.9062 - val_loss: 0.2277 - val_binary_accuracy: 0.9063\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2217 - binary_accuracy: 0.9080\n",
      "Epoch 5: val_loss improved from 0.22766 to 0.22530, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 316s 718ms/step - loss: 0.2217 - binary_accuracy: 0.9080 - val_loss: 0.2253 - val_binary_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2181 - binary_accuracy: 0.9092\n",
      "Epoch 6: val_loss improved from 0.22530 to 0.22389, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 305s 693ms/step - loss: 0.2181 - binary_accuracy: 0.9092 - val_loss: 0.2239 - val_binary_accuracy: 0.9077\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2152 - binary_accuracy: 0.9108\n",
      "Epoch 7: val_loss improved from 0.22389 to 0.22283, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 355s 807ms/step - loss: 0.2152 - binary_accuracy: 0.9108 - val_loss: 0.2228 - val_binary_accuracy: 0.9081\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2119 - binary_accuracy: 0.9118\n",
      "Epoch 8: val_loss improved from 0.22283 to 0.22213, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 299s 680ms/step - loss: 0.2119 - binary_accuracy: 0.9118 - val_loss: 0.2221 - val_binary_accuracy: 0.9083\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2101 - binary_accuracy: 0.9126\n",
      "Epoch 9: val_loss improved from 0.22213 to 0.22183, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 315s 715ms/step - loss: 0.2101 - binary_accuracy: 0.9126 - val_loss: 0.2218 - val_binary_accuracy: 0.9083\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2082 - binary_accuracy: 0.9135\n",
      "Epoch 10: val_loss improved from 0.22183 to 0.22109, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 305s 692ms/step - loss: 0.2082 - binary_accuracy: 0.9135 - val_loss: 0.2211 - val_binary_accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "history_model_2 = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.save(\"../trained_models/model_2_fine_grained_vgg.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net.trainable = True  \n",
    "\n",
    "for layer in efficient_net.layers[:-10]:\n",
    "    layer.trainable = False \n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained_model2_tune/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2235 - binary_accuracy: 0.9072\n",
      "Epoch 10: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 465s 1s/step - loss: 0.2235 - binary_accuracy: 0.9072 - val_loss: 0.2200 - val_binary_accuracy: 0.9092\n",
      "Epoch 11/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1878 - binary_accuracy: 0.9226\n",
      "Epoch 11: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 455s 1s/step - loss: 0.1878 - binary_accuracy: 0.9226 - val_loss: 0.2128 - val_binary_accuracy: 0.9135\n",
      "Epoch 12/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1663 - binary_accuracy: 0.9322\n",
      "Epoch 12: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 471s 1s/step - loss: 0.1663 - binary_accuracy: 0.9322 - val_loss: 0.2175 - val_binary_accuracy: 0.9121\n",
      "Epoch 13/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1477 - binary_accuracy: 0.9406\n",
      "Epoch 13: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 465s 1s/step - loss: 0.1477 - binary_accuracy: 0.9406 - val_loss: 0.2199 - val_binary_accuracy: 0.9129\n",
      "Epoch 14/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1308 - binary_accuracy: 0.9487\n",
      "Epoch 14: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 455s 1s/step - loss: 0.1308 - binary_accuracy: 0.9487 - val_loss: 0.2306 - val_binary_accuracy: 0.9093\n",
      "Epoch 15/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1161 - binary_accuracy: 0.9549\n",
      "Epoch 15: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 474s 1s/step - loss: 0.1161 - binary_accuracy: 0.9549 - val_loss: 0.2385 - val_binary_accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "history_model_2_fine = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=15, \n",
    "        initial_epoch=history_model_2.epoch[-1], # start from previous last epoch \n",
    "        callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.save(\"../trained_models/fine_model_2_fine_grained_vgg.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: EfficientNet with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "data_augmentation_model = Sequential([\n",
    "    preprocessing.RandomHeight(0.2),\n",
    "    preprocessing.RandomWidth(0.2), \n",
    "    preprocessing.RandomZoom(0.2)\n",
    "], name=\"data_augmentation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "efficient_net_model_3 = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "efficient_net_model_3.trainable = False \n",
    "\n",
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3)) \n",
    "x = data_augmentation_model(inputs)\n",
    "x = efficient_net_model_3(x, training=False) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_3 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 224, 224, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 26)                33306     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,082,877\n",
      "Trainable params: 33,306\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/model_3_fine_grained/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2914 - binary_accuracy: 0.8799  \n",
      "Epoch 1: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 65091s 149s/step - loss: 0.2914 - binary_accuracy: 0.8799 - val_loss: 0.2528 - val_binary_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2468 - binary_accuracy: 0.8978\n",
      "Epoch 2: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 264s 596ms/step - loss: 0.2468 - binary_accuracy: 0.8978 - val_loss: 0.2395 - val_binary_accuracy: 0.9003\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2367 - binary_accuracy: 0.9016\n",
      "Epoch 3: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 241s 552ms/step - loss: 0.2367 - binary_accuracy: 0.9016 - val_loss: 0.2337 - val_binary_accuracy: 0.9028\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2305 - binary_accuracy: 0.9043\n",
      "Epoch 4: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 266s 601ms/step - loss: 0.2305 - binary_accuracy: 0.9043 - val_loss: 0.2298 - val_binary_accuracy: 0.9046\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2257 - binary_accuracy: 0.9061\n",
      "Epoch 5: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 263s 594ms/step - loss: 0.2257 - binary_accuracy: 0.9061 - val_loss: 0.2280 - val_binary_accuracy: 0.9053\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2219 - binary_accuracy: 0.9076\n",
      "Epoch 6: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 248s 565ms/step - loss: 0.2219 - binary_accuracy: 0.9076 - val_loss: 0.2256 - val_binary_accuracy: 0.9061\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2193 - binary_accuracy: 0.9084\n",
      "Epoch 7: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 250s 570ms/step - loss: 0.2193 - binary_accuracy: 0.9084 - val_loss: 0.2250 - val_binary_accuracy: 0.9067\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2167 - binary_accuracy: 0.9098\n",
      "Epoch 8: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 243s 554ms/step - loss: 0.2167 - binary_accuracy: 0.9098 - val_loss: 0.2242 - val_binary_accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2145 - binary_accuracy: 0.9109\n",
      "Epoch 9: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 267s 605ms/step - loss: 0.2145 - binary_accuracy: 0.9109 - val_loss: 0.2234 - val_binary_accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2134 - binary_accuracy: 0.9111\n",
      "Epoch 10: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 271s 613ms/step - loss: 0.2134 - binary_accuracy: 0.9111 - val_loss: 0.2233 - val_binary_accuracy: 0.9069\n"
     ]
    }
   ],
   "source": [
    "history_model_3 = model_3.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10, \n",
    "        callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3.save(\"../trained_models/model_3_fine_grained.h5\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models \n",
    "\n",
    "### Evaluation of Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_loaded = tf.keras.models.load_model(\"../trained_models/model_1_fine_grained_vgg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1a3822b6ee0>,\n",
       " <keras.layers.core.tf_op_layer.TFOpLambda at 0x1a382a2c2e0>,\n",
       " <keras.layers.core.tf_op_layer.SlicingOpLambda at 0x1a382a2c490>,\n",
       " <keras.layers.core.tf_op_layer.TFOpLambda at 0x1a382a2c670>,\n",
       " <keras.engine.functional.Functional at 0x1a382a2ca30>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x1a382a4b0a0>,\n",
       " <keras.layers.core.dense.Dense at 0x1a382a7e5b0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_loaded.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_similar-products",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2ae175228e4b907f6759aae1200b3a8fda51833cb8d87ddc125794bd32e5598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
