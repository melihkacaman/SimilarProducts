{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"../datasets/attribute_set/train_val_data_fine_grained_all.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16000 entries, 0 to 1999\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paths            16000 non-null  object\n",
      " 1   floral           16000 non-null  int64 \n",
      " 2   graphic          16000 non-null  int64 \n",
      " 3   striped          16000 non-null  int64 \n",
      " 4   embroidered      16000 non-null  int64 \n",
      " 5   pleated          16000 non-null  int64 \n",
      " 6   solid            16000 non-null  int64 \n",
      " 7   lattice          16000 non-null  int64 \n",
      " 8   long_sleeve      16000 non-null  int64 \n",
      " 9   short_sleeve     16000 non-null  int64 \n",
      " 10  sleeveless       16000 non-null  int64 \n",
      " 11  maxi_length      16000 non-null  int64 \n",
      " 12  mini_length      16000 non-null  int64 \n",
      " 13  no_dress         16000 non-null  int64 \n",
      " 14  crew_neckline    16000 non-null  int64 \n",
      " 15  v_neckline       16000 non-null  int64 \n",
      " 16  square_neckline  16000 non-null  int64 \n",
      " 17  no_neckline      16000 non-null  int64 \n",
      " 18  denim            16000 non-null  int64 \n",
      " 19  chiffon          16000 non-null  int64 \n",
      " 20  cotton           16000 non-null  int64 \n",
      " 21  leather          16000 non-null  int64 \n",
      " 22  faux             16000 non-null  int64 \n",
      " 23  knit             16000 non-null  int64 \n",
      " 24  tight            16000 non-null  int64 \n",
      " 25  loose            16000 non-null  int64 \n",
      " 26  conventional     16000 non-null  int64 \n",
      "dtypes: int64(26), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>floral</th>\n",
       "      <th>graphic</th>\n",
       "      <th>striped</th>\n",
       "      <th>embroidered</th>\n",
       "      <th>pleated</th>\n",
       "      <th>solid</th>\n",
       "      <th>lattice</th>\n",
       "      <th>long_sleeve</th>\n",
       "      <th>short_sleeve</th>\n",
       "      <th>...</th>\n",
       "      <th>no_neckline</th>\n",
       "      <th>denim</th>\n",
       "      <th>chiffon</th>\n",
       "      <th>cotton</th>\n",
       "      <th>leather</th>\n",
       "      <th>faux</th>\n",
       "      <th>knit</th>\n",
       "      <th>tight</th>\n",
       "      <th>loose</th>\n",
       "      <th>conventional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Sweet_Crochet_B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Classic_Pencil_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Strapless_Diamo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Mid-Rise_-_Acid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/big_ds/img-001/img/Zippered_Single...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paths  floral  graphic  \\\n",
       "0  ../datasets/big_ds/img-001/img/Sweet_Crochet_B...       0        0   \n",
       "1  ../datasets/big_ds/img-001/img/Classic_Pencil_...       0        0   \n",
       "2  ../datasets/big_ds/img-001/img/Strapless_Diamo...       0        1   \n",
       "3  ../datasets/big_ds/img-001/img/Mid-Rise_-_Acid...       0        0   \n",
       "4  ../datasets/big_ds/img-001/img/Zippered_Single...       0        0   \n",
       "\n",
       "   striped  embroidered  pleated  solid  lattice  long_sleeve  short_sleeve  \\\n",
       "0        0            1        0      0        0            0             0   \n",
       "1        0            0        0      1        0            0             0   \n",
       "2        0            0        0      0        0            0             0   \n",
       "3        0            0        0      1        0            0             0   \n",
       "4        0            0        0      1        0            1             0   \n",
       "\n",
       "   ...  no_neckline  denim  chiffon  cotton  leather  faux  knit  tight  \\\n",
       "0  ...            1      0        1       0        0     0     0      0   \n",
       "1  ...            1      0        0       1        0     0     0      1   \n",
       "2  ...            1      0        0       1        0     0     0      0   \n",
       "3  ...            1      1        0       0        0     0     0      1   \n",
       "4  ...            0      0        0       1        0     0     0      0   \n",
       "\n",
       "   loose  conventional  \n",
       "0      0             1  \n",
       "1      0             0  \n",
       "2      0             1  \n",
       "3      0             0  \n",
       "4      0             1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 14000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Pipeline by using tf.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['../datasets/big_ds/img-001/img/Sweet_Crochet_Blouse/img_00000070.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Classic_Pencil_Skirt/img_00000010.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Strapless_Diamond_Print_Dress/img_00000038.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Mid-Rise_-_Acid_Wash_Skinny_Jeans/img_00000010.jpg',\n",
       "        '../datasets/big_ds/img-001/img/Zippered_Single-Button_Blazer/img_00000078.jpg'],\n",
       "       dtype=object),\n",
       " 16000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = data.paths.to_numpy()  \n",
    "fnames[:5], len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "ds_size = data.shape[0] \n",
    "number_of_selected_samples = 2000 \n",
    "\n",
    "# filelist_ds = tf.data.Dataset.from_tensor_slices(fnames[:number_of_selected_samples]) \n",
    "filelist_ds = tf.data.Dataset.from_tensor_slices(fnames) \n",
    "\n",
    "\n",
    "filelist_ds.cardinality().numpy() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    \"\"\"\n",
    "        file_path: the file path for the image that you want to select\n",
    "    \"\"\"\n",
    "    labels = data.loc[data.paths == file_path].to_numpy().squeeze()[1:].astype(\"int64\")\n",
    "    return tf.convert_to_tensor(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(26,), dtype=int64, numpy=\n",
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label(fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and scale the images so that we can save time in training  \n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 \n",
    "def decode_img(img):\n",
    "    \"\"\"\n",
    "        img: img is the image \n",
    "    \"\"\" \n",
    "    #color images \n",
    "    img = tf.image.decode_jpeg(img, channels=3) \n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)  #convert unit8 tensor to floats in the [0,1] range\n",
    "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \n",
    "    # img = img / tf.constant(256, dtype=tf.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images_labels(file_path: tf.Tensor): \n",
    "    label = get_label(file_path) \n",
    "    img = tf.io.read_file(file_path) \n",
    "    img = decode_img(img) \n",
    "    return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_images_labels(fnames[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = filelist_ds.take(TRAIN_SIZE) \n",
    "ds_test = filelist_ds.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process All the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(lambda x: \n",
    "                        tf.py_function(func=combine_images_labels, \n",
    "                                       inp=[x], # input of the function \n",
    "                                       Tout=(tf.float32,tf.int64)),  # return type \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE, # parallelizing data extraction \n",
    "                        deterministic=False \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test= ds_test.map(lambda x: tf.py_function(func=combine_images_labels,\n",
    "          inp=[x], Tout=(tf.float32,tf.int64)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Pipeline \n",
    "\n",
    "- **batch**(): Combines consecutive elements of this dataset into batches.\n",
    "- **cache**(): Caches the elements in this dataset. he first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.Subsequent iterations will use the cached data.\n",
    "- **prefetch**(): Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to *prefetch*. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_batched = ds_train.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) \n",
    "ds_test_batched = ds_test.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_batched.cardinality().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes = len(data.columns) - 1  \n",
    "nr_of_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1:Fine Grained VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "base_model = keras.applications.VGG16(\n",
    "    weights=\"imagenet\", # load weights pre-trained on ImageNet. \n",
    "    input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), # VGG16 expects min 32 x 32 \n",
    "    include_top = False # do not include output layer of the image net vgg \n",
    ")\n",
    "base_model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
    "x = tf.cast(inputs, tf.float32)\n",
    "x = tf.keras.applications.vgg16.preprocess_input(x)   \n",
    "x = base_model(x) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_1 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) \n",
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=True,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.4559 - binary_accuracy: 0.8399  \n",
      "Epoch 1: val_loss improved from inf to 0.32555, saving model to checkpoints/attribute_prediction_classifier_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 66903s 153s/step - loss: 0.4559 - binary_accuracy: 0.8399 - val_loss: 0.3255 - val_binary_accuracy: 0.8716\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2981 - binary_accuracy: 0.8807\n",
      "Epoch 2: val_loss improved from 0.32555 to 0.29041, saving model to checkpoints/attribute_prediction_classifier_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 820s 2s/step - loss: 0.2981 - binary_accuracy: 0.8807 - val_loss: 0.2904 - val_binary_accuracy: 0.8829\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2720 - binary_accuracy: 0.8890\n",
      "Epoch 3: val_loss improved from 0.29041 to 0.27955, saving model to checkpoints/attribute_prediction_classifier_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 815s 2s/step - loss: 0.2720 - binary_accuracy: 0.8890 - val_loss: 0.2795 - val_binary_accuracy: 0.8858\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2611 - binary_accuracy: 0.8928\n",
      "Epoch 4: val_loss improved from 0.27955 to 0.27580, saving model to checkpoints/attribute_prediction_classifier_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 815s 2s/step - loss: 0.2611 - binary_accuracy: 0.8928 - val_loss: 0.2758 - val_binary_accuracy: 0.8869\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2556 - binary_accuracy: 0.8947\n",
      "Epoch 5: val_loss improved from 0.27580 to 0.27475, saving model to checkpoints/attribute_prediction_classifier_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 815s 2s/step - loss: 0.2556 - binary_accuracy: 0.8947 - val_loss: 0.2748 - val_binary_accuracy: 0.8872\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2525 - binary_accuracy: 0.8957\n",
      "Epoch 6: val_loss did not improve from 0.27475\n",
      "438/438 [==============================] - 810s 2s/step - loss: 0.2525 - binary_accuracy: 0.8957 - val_loss: 0.2748 - val_binary_accuracy: 0.8881\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2506 - binary_accuracy: 0.8964\n",
      "Epoch 7: val_loss did not improve from 0.27475\n",
      "438/438 [==============================] - 894s 2s/step - loss: 0.2506 - binary_accuracy: 0.8964 - val_loss: 0.2754 - val_binary_accuracy: 0.8878\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2494 - binary_accuracy: 0.8968\n",
      "Epoch 8: val_loss did not improve from 0.27475\n",
      "438/438 [==============================] - 801s 2s/step - loss: 0.2494 - binary_accuracy: 0.8968 - val_loss: 0.2761 - val_binary_accuracy: 0.8873\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2486 - binary_accuracy: 0.8970\n",
      "Epoch 9: val_loss did not improve from 0.27475\n",
      "438/438 [==============================] - 778s 2s/step - loss: 0.2486 - binary_accuracy: 0.8970 - val_loss: 0.2769 - val_binary_accuracy: 0.8871\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2479 - binary_accuracy: 0.8972\n",
      "Epoch 10: val_loss did not improve from 0.27475\n",
      "438/438 [==============================] - 770s 2s/step - loss: 0.2479 - binary_accuracy: 0.8972 - val_loss: 0.2777 - val_binary_accuracy: 0.8872\n"
     ]
    }
   ],
   "source": [
    "history_model_1 = model_1.fit(ds_train_batched, \n",
    "                            epochs=10,\n",
    "                            validation_data =ds_test_batched,\n",
    "                            callbacks=[early_stopping, checkpoint_callback]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1.save(\"../trained_models/model_1_fine_grained_vgg.h5\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Efficient Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "efficient_net.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3)) \n",
    "x = efficient_net(inputs) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_2 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                33306     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,082,877\n",
      "Trainable params: 33,306\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) \n",
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained_model3/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=True,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2891 - binary_accuracy: 0.8815\n",
      "Epoch 1: val_loss improved from inf to 0.24964, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 294s 650ms/step - loss: 0.2891 - binary_accuracy: 0.8815 - val_loss: 0.2496 - val_binary_accuracy: 0.8974\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2440 - binary_accuracy: 0.8994\n",
      "Epoch 2: val_loss improved from 0.24964 to 0.23676, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 317s 721ms/step - loss: 0.2440 - binary_accuracy: 0.8994 - val_loss: 0.2368 - val_binary_accuracy: 0.9028\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2332 - binary_accuracy: 0.9036\n",
      "Epoch 3: val_loss improved from 0.23676 to 0.23082, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 316s 718ms/step - loss: 0.2332 - binary_accuracy: 0.9036 - val_loss: 0.2308 - val_binary_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2265 - binary_accuracy: 0.9062\n",
      "Epoch 4: val_loss improved from 0.23082 to 0.22766, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 313s 710ms/step - loss: 0.2265 - binary_accuracy: 0.9062 - val_loss: 0.2277 - val_binary_accuracy: 0.9063\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2217 - binary_accuracy: 0.9080\n",
      "Epoch 5: val_loss improved from 0.22766 to 0.22530, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 316s 718ms/step - loss: 0.2217 - binary_accuracy: 0.9080 - val_loss: 0.2253 - val_binary_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2181 - binary_accuracy: 0.9092\n",
      "Epoch 6: val_loss improved from 0.22530 to 0.22389, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 305s 693ms/step - loss: 0.2181 - binary_accuracy: 0.9092 - val_loss: 0.2239 - val_binary_accuracy: 0.9077\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2152 - binary_accuracy: 0.9108\n",
      "Epoch 7: val_loss improved from 0.22389 to 0.22283, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 355s 807ms/step - loss: 0.2152 - binary_accuracy: 0.9108 - val_loss: 0.2228 - val_binary_accuracy: 0.9081\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2119 - binary_accuracy: 0.9118\n",
      "Epoch 8: val_loss improved from 0.22283 to 0.22213, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 299s 680ms/step - loss: 0.2119 - binary_accuracy: 0.9118 - val_loss: 0.2221 - val_binary_accuracy: 0.9083\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2101 - binary_accuracy: 0.9126\n",
      "Epoch 9: val_loss improved from 0.22213 to 0.22183, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 315s 715ms/step - loss: 0.2101 - binary_accuracy: 0.9126 - val_loss: 0.2218 - val_binary_accuracy: 0.9083\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2082 - binary_accuracy: 0.9135\n",
      "Epoch 10: val_loss improved from 0.22183 to 0.22109, saving model to checkpoints/attribute_prediction_classifier_fine_grained_model3\\checkpoint.ckpt\n",
      "438/438 [==============================] - 305s 692ms/step - loss: 0.2082 - binary_accuracy: 0.9135 - val_loss: 0.2211 - val_binary_accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "history_model_2 = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.save(\"../trained_models/model_2_fine_grained_vgg.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net.trainable = True  \n",
    "\n",
    "for layer in efficient_net.layers[:-10]:\n",
    "    layer.trainable = False \n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/attribute_prediction_classifier_fine_grained_model2_tune/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2235 - binary_accuracy: 0.9072\n",
      "Epoch 10: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 465s 1s/step - loss: 0.2235 - binary_accuracy: 0.9072 - val_loss: 0.2200 - val_binary_accuracy: 0.9092\n",
      "Epoch 11/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1878 - binary_accuracy: 0.9226\n",
      "Epoch 11: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 455s 1s/step - loss: 0.1878 - binary_accuracy: 0.9226 - val_loss: 0.2128 - val_binary_accuracy: 0.9135\n",
      "Epoch 12/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1663 - binary_accuracy: 0.9322\n",
      "Epoch 12: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 471s 1s/step - loss: 0.1663 - binary_accuracy: 0.9322 - val_loss: 0.2175 - val_binary_accuracy: 0.9121\n",
      "Epoch 13/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1477 - binary_accuracy: 0.9406\n",
      "Epoch 13: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 465s 1s/step - loss: 0.1477 - binary_accuracy: 0.9406 - val_loss: 0.2199 - val_binary_accuracy: 0.9129\n",
      "Epoch 14/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1308 - binary_accuracy: 0.9487\n",
      "Epoch 14: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 455s 1s/step - loss: 0.1308 - binary_accuracy: 0.9487 - val_loss: 0.2306 - val_binary_accuracy: 0.9093\n",
      "Epoch 15/15\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.1161 - binary_accuracy: 0.9549\n",
      "Epoch 15: saving model to checkpoints/attribute_prediction_classifier_fine_grained_model2_tune\\checkpoint.ckpt\n",
      "438/438 [==============================] - 474s 1s/step - loss: 0.1161 - binary_accuracy: 0.9549 - val_loss: 0.2385 - val_binary_accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "history_model_2_fine = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=15, \n",
    "        initial_epoch=history_model_2.epoch[-1], # start from previous last epoch \n",
    "        callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.save(\"../trained_models/fine_model_2_fine_grained_vgg.h5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: EfficientNet with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "data_augmentation_model = Sequential([\n",
    "    preprocessing.RandomHeight(0.2),\n",
    "    preprocessing.RandomWidth(0.2), \n",
    "    preprocessing.RandomZoom(0.2)\n",
    "], name=\"data_augmentation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "efficient_net_model_3 = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "efficient_net_model_3.trainable = False \n",
    "\n",
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3)) \n",
    "x = data_augmentation_model(inputs)\n",
    "x = efficient_net_model_3(x, training=False) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_3 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 224, 224, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 26)                33306     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,082,877\n",
      "Trainable params: 33,306\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/model_3_fine_grained/checkpoint.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2914 - binary_accuracy: 0.8799  \n",
      "Epoch 1: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 65091s 149s/step - loss: 0.2914 - binary_accuracy: 0.8799 - val_loss: 0.2528 - val_binary_accuracy: 0.8951\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2468 - binary_accuracy: 0.8978\n",
      "Epoch 2: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 264s 596ms/step - loss: 0.2468 - binary_accuracy: 0.8978 - val_loss: 0.2395 - val_binary_accuracy: 0.9003\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2367 - binary_accuracy: 0.9016\n",
      "Epoch 3: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 241s 552ms/step - loss: 0.2367 - binary_accuracy: 0.9016 - val_loss: 0.2337 - val_binary_accuracy: 0.9028\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2305 - binary_accuracy: 0.9043\n",
      "Epoch 4: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 266s 601ms/step - loss: 0.2305 - binary_accuracy: 0.9043 - val_loss: 0.2298 - val_binary_accuracy: 0.9046\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2257 - binary_accuracy: 0.9061\n",
      "Epoch 5: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 263s 594ms/step - loss: 0.2257 - binary_accuracy: 0.9061 - val_loss: 0.2280 - val_binary_accuracy: 0.9053\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2219 - binary_accuracy: 0.9076\n",
      "Epoch 6: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 248s 565ms/step - loss: 0.2219 - binary_accuracy: 0.9076 - val_loss: 0.2256 - val_binary_accuracy: 0.9061\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2193 - binary_accuracy: 0.9084\n",
      "Epoch 7: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 250s 570ms/step - loss: 0.2193 - binary_accuracy: 0.9084 - val_loss: 0.2250 - val_binary_accuracy: 0.9067\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2167 - binary_accuracy: 0.9098\n",
      "Epoch 8: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 243s 554ms/step - loss: 0.2167 - binary_accuracy: 0.9098 - val_loss: 0.2242 - val_binary_accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2145 - binary_accuracy: 0.9109\n",
      "Epoch 9: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 267s 605ms/step - loss: 0.2145 - binary_accuracy: 0.9109 - val_loss: 0.2234 - val_binary_accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - ETA: 0s - loss: 0.2134 - binary_accuracy: 0.9111\n",
      "Epoch 10: saving model to checkpoints/model_3_fine_grained\\checkpoint.ckpt\n",
      "438/438 [==============================] - 271s 613ms/step - loss: 0.2134 - binary_accuracy: 0.9111 - val_loss: 0.2233 - val_binary_accuracy: 0.9069\n"
     ]
    }
   ],
   "source": [
    "history_model_3 = model_3.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10, \n",
    "        callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3.save(\"../trained_models/model_3_fine_grained.h5\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_similar-products')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9fdba9d57aea1bb5c6986083e3594030ae55b020a0216061e1820b16640168"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
