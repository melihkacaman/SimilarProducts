{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"../datasets/attribute_set/in_shop_attr/in_shop_data_all.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 53590 entries, 0 to 53589\n",
      "Columns: 350 entries, path to 361_cutout\n",
      "dtypes: int64(347), object(3)\n",
      "memory usage: 143.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>0_lightweight</th>\n",
       "      <th>1_polyester</th>\n",
       "      <th>2_woven</th>\n",
       "      <th>3_knit</th>\n",
       "      <th>4_cotton</th>\n",
       "      <th>5_unlined</th>\n",
       "      <th>6_rayon</th>\n",
       "      <th>...</th>\n",
       "      <th>352_vest</th>\n",
       "      <th>353_accessorie</th>\n",
       "      <th>354_brunch</th>\n",
       "      <th>355_girly</th>\n",
       "      <th>356_open-knit</th>\n",
       "      <th>357_polka</th>\n",
       "      <th>358_gauze</th>\n",
       "      <th>359_grey</th>\n",
       "      <th>360_outhwestern</th>\n",
       "      <th>361_cutout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/D...</td>\n",
       "      <td>id_00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/D...</td>\n",
       "      <td>id_00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/D...</td>\n",
       "      <td>id_00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/D...</td>\n",
       "      <td>id_00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/S...</td>\n",
       "      <td>id_00000003</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path           id  group  \\\n",
       "0  ../datasets/big_ds/img-in_shop/img/img/WOMEN/D...  id_00000002  train   \n",
       "1  ../datasets/big_ds/img-in_shop/img/img/WOMEN/D...  id_00000002  train   \n",
       "2  ../datasets/big_ds/img-in_shop/img/img/WOMEN/D...  id_00000002  train   \n",
       "3  ../datasets/big_ds/img-in_shop/img/img/WOMEN/D...  id_00000002  train   \n",
       "4  ../datasets/big_ds/img-in_shop/img/img/WOMEN/S...  id_00000003  train   \n",
       "\n",
       "   0_lightweight  1_polyester  2_woven  3_knit  4_cotton  5_unlined  6_rayon  \\\n",
       "0              1            0        1       0         0          1        1   \n",
       "1              1            0        1       0         0          1        1   \n",
       "2              1            0        1       0         0          1        1   \n",
       "3              1            0        1       0         0          1        1   \n",
       "4              1            1        1       0         0          0        1   \n",
       "\n",
       "   ...  352_vest  353_accessorie  354_brunch  355_girly  356_open-knit  \\\n",
       "0  ...         0               0           0          0              0   \n",
       "1  ...         0               0           0          0              0   \n",
       "2  ...         0               0           0          0              0   \n",
       "3  ...         0               0           0          0              0   \n",
       "4  ...         0               0           0          0              0   \n",
       "\n",
       "   357_polka  358_gauze  359_grey  360_outhwestern  361_cutout  \n",
       "0          0          0         0                0           0  \n",
       "1          0          0         0                0           0  \n",
       "2          0          0         0                0           0  \n",
       "3          0          0         0                0           0  \n",
       "4          0          0         0                0           0  \n",
       "\n",
       "[5 rows x 350 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>0_lightweight</th>\n",
       "      <th>1_polyester</th>\n",
       "      <th>2_woven</th>\n",
       "      <th>3_knit</th>\n",
       "      <th>4_cotton</th>\n",
       "      <th>5_unlined</th>\n",
       "      <th>6_rayon</th>\n",
       "      <th>...</th>\n",
       "      <th>352_vest</th>\n",
       "      <th>353_accessorie</th>\n",
       "      <th>354_brunch</th>\n",
       "      <th>355_girly</th>\n",
       "      <th>356_open-knit</th>\n",
       "      <th>357_polka</th>\n",
       "      <th>358_gauze</th>\n",
       "      <th>359_grey</th>\n",
       "      <th>360_outhwestern</th>\n",
       "      <th>361_cutout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53585</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/J...</td>\n",
       "      <td>id_00007982</td>\n",
       "      <td>gallery</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53586</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/J...</td>\n",
       "      <td>id_00007982</td>\n",
       "      <td>query</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53587</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/J...</td>\n",
       "      <td>id_00007982</td>\n",
       "      <td>gallery</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53588</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/J...</td>\n",
       "      <td>id_00007982</td>\n",
       "      <td>query</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53589</th>\n",
       "      <td>../datasets/big_ds/img-in_shop/img/img/WOMEN/J...</td>\n",
       "      <td>id_00007982</td>\n",
       "      <td>query</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path           id  \\\n",
       "53585  ../datasets/big_ds/img-in_shop/img/img/WOMEN/J...  id_00007982   \n",
       "53586  ../datasets/big_ds/img-in_shop/img/img/WOMEN/J...  id_00007982   \n",
       "53587  ../datasets/big_ds/img-in_shop/img/img/WOMEN/J...  id_00007982   \n",
       "53588  ../datasets/big_ds/img-in_shop/img/img/WOMEN/J...  id_00007982   \n",
       "53589  ../datasets/big_ds/img-in_shop/img/img/WOMEN/J...  id_00007982   \n",
       "\n",
       "         group  0_lightweight  1_polyester  2_woven  3_knit  4_cotton  \\\n",
       "53585  gallery              1            1        0       1         0   \n",
       "53586    query              1            1        0       1         0   \n",
       "53587  gallery              1            1        0       1         0   \n",
       "53588    query              1            1        0       1         0   \n",
       "53589    query              1            1        0       1         0   \n",
       "\n",
       "       5_unlined  6_rayon  ...  352_vest  353_accessorie  354_brunch  \\\n",
       "53585          0        1  ...         0               0           0   \n",
       "53586          0        1  ...         0               0           0   \n",
       "53587          0        1  ...         0               0           0   \n",
       "53588          0        1  ...         0               0           0   \n",
       "53589          0        1  ...         0               0           0   \n",
       "\n",
       "       355_girly  356_open-knit  357_polka  358_gauze  359_grey  \\\n",
       "53585          0              0          0          0         0   \n",
       "53586          0              0          0          0         0   \n",
       "53587          0              0          0          0         0   \n",
       "53588          0              0          0          0         0   \n",
       "53589          0              0          0          0         0   \n",
       "\n",
       "       360_outhwestern  361_cutout  \n",
       "53585                0           0  \n",
       "53586                0           0  \n",
       "53587                0           0  \n",
       "53588                0           0  \n",
       "53589                0           0  \n",
       "\n",
       "[5 rows x 350 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE = data[data[\"group\"] == \"train\"].shape[0]\n",
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14000, 350), (12338, 350))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_first = data[data[\"group\"] == \"train\"].iloc[:14000, :]\n",
    "train_data_second = data[data[\"group\"] == \"train\"].iloc[14000:, :].sample(frac=1).reset_index(drop=True)\n",
    "train_data_first.shape, train_data_second.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12811"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"group\"] == \"gallery\"].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 350), (2000, 350))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_first = data[data[\"group\"] == \"gallery\"].iloc[:2000,:] \n",
    "val_data_second = data[data[\"group\"] == \"gallery\"].iloc[2000:4000, :] \n",
    "val_data_first.shape, val_data_second.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14441, 350)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"group\"] == \"query\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 350), (14338, 350))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_first = pd.concat([train_data_first, val_data_first]) \n",
    "data_second = pd.concat([train_data_second, val_data_second]) \n",
    "data_first.shape, data_second.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52712, 350)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_second = data_second.drop_duplicates()\n",
    "data = data_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['../datasets/big_ds/img-in_shop/img/img/WOMEN/Blouses_Shirts/id_00004472/02_3_back.jpg',\n",
       "        '../datasets/big_ds/img-in_shop/img/img/WOMEN/Tees_Tanks/id_00007277/01_3_back.jpg',\n",
       "        '../datasets/big_ds/img-in_shop/img/img/WOMEN/Shorts/id_00004829/05_4_full.jpg',\n",
       "        '../datasets/big_ds/img-in_shop/img/img/WOMEN/Blouses_Shirts/id_00005469/01_4_full.jpg',\n",
       "        '../datasets/big_ds/img-in_shop/img/img/WOMEN/Dresses/id_00007154/02_7_additional.jpg'],\n",
       "       dtype=object),\n",
       " 14179)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = data.path.to_numpy()  \n",
    "fnames[:5], len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "ds_size = data.shape[0] \n",
    "number_of_selected_samples = 500 \n",
    "\n",
    "#filelist_ds = tf.data.Dataset.from_tensor_slices(fnames[:number_of_selected_samples]) \n",
    "filelist_ds = tf.data.Dataset.from_tensor_slices(fnames) \n",
    "\n",
    "\n",
    "filelist_ds.cardinality().numpy() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    \"\"\"\n",
    "        file_path: the file path for the image that you want to select\n",
    "    \"\"\"\n",
    "    labels = data.loc[data.path == file_path].to_numpy().squeeze()[3:].astype(\"int64\")\n",
    "    return tf.convert_to_tensor(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(347,), dtype=int64, numpy=\n",
       "array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label(fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and scale the images so that we can save time in training  \n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 \n",
    "def decode_img(img):\n",
    "    \"\"\"\n",
    "        img: img is the image \n",
    "    \"\"\" \n",
    "    #color images \n",
    "    img = tf.image.decode_jpeg(img, channels=3) \n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)  #convert unit8 tensor to floats in the [0,1] range\n",
    "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \n",
    "    # img = img / tf.constant(256, dtype=tf.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images_labels(file_path: tf.Tensor): \n",
    "    label = get_label(file_path) \n",
    "    img = tf.io.read_file(file_path) \n",
    "    img = decode_img(img) \n",
    "    return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_images_labels(fnames[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14179, 350)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 12338\n",
    "data_size = data.shape[0]\n",
    "ds_train = filelist_ds.take(split_size) \n",
    "ds_test = filelist_ds.skip(split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process All the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(lambda x: \n",
    "                        tf.py_function(func=combine_images_labels, \n",
    "                                       inp=[x], # input of the function \n",
    "                                       Tout=(tf.float32,tf.int64)),  # return type \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE, # parallelizing data extraction \n",
    "                        deterministic=False \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test= ds_test.map(lambda x: tf.py_function(func=combine_images_labels,\n",
    "          inp=[x], Tout=(tf.float32,tf.int64)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Pipeline \n",
    "\n",
    "- **batch**(): Combines consecutive elements of this dataset into batches.\n",
    "- **cache**(): Caches the elements in this dataset. he first time the dataset is iterated over, its elements will be cached either in the specified file or in memory.Subsequent iterations will use the cached data.\n",
    "- **prefetch**(): Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to *prefetch*. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_batched = ds_train.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) \n",
    "ds_test_batched = ds_test.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_batched.cardinality().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42176"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1318 * 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_of_classes = len(data.columns) - 3  \n",
    "nr_of_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0: simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)) \n",
    "x = keras.layers.Conv2D(filters=32,\n",
    "                        kernel_size=5,\n",
    "                        activation='relu')(inputs) \n",
    "x = keras.layers.Conv2D(filters=32,\n",
    "                        kernel_size=5,\n",
    "                        activation='relu')(x) \n",
    "x = keras.layers.GlobalMaxPool2D(name=\"max_pooling\")(x) \n",
    "outputs = keras.layers.Dense(nr_of_classes, name=\"output_of_cnn\")(x)\n",
    "\n",
    "deneme_model = tf.keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme_model.compile(optimizer=keras.optimizers.Adam(), \n",
    "                    loss=keras.losses.BinaryCrossentropy(),\n",
    "                    metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1318/1318 [==============================] - ETA: 0s - loss: 5.4009 - binary_accuracy: 0.6461  "
     ]
    }
   ],
   "source": [
    "history_model_den = deneme_model.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1:Fine Grained VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "base_model = keras.applications.VGG16(\n",
    "    weights=\"imagenet\", # load weights pre-trained on ImageNet. \n",
    "    input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), # VGG16 expects min 32 x 32 \n",
    "    include_top = False # do not include output layer of the image net vgg \n",
    ")\n",
    "base_model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
    "x = tf.cast(inputs, tf.float32)\n",
    "x = tf.keras.applications.vgg16.preprocess_input(x)   \n",
    "x = base_model(x) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_1 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_checkpoint = \"checkpoints/in-shop_model_1/checkpoint_VGG_Model_1.ckpt\"\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) \n",
    "checkpoint_path = \"checkpoints/in-shop_model_1_STEP2/checkpoint_VGG_Model_1.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23077b103a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.load_weights(previous_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1660 - tp: 62641.0000 - fp: 41268.0000 - tn: 4003056.0000 - fn: 174321.0000 - accuracy: 0.9496 - precision: 0.6028 - recall: 0.2644  \n",
      "Epoch 1: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 52140s 135s/step - loss: 0.1660 - tp: 62641.0000 - fp: 41268.0000 - tn: 4003056.0000 - fn: 174321.0000 - accuracy: 0.9496 - precision: 0.6028 - recall: 0.2644 - val_loss: 0.1621 - val_tp: 9222.0000 - val_fp: 5312.0000 - val_tn: 598846.0000 - val_fn: 25447.0000 - val_accuracy: 0.9519 - val_precision: 0.6345 - val_recall: 0.2660\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1509 - tp: 65404.0000 - fp: 34301.0000 - tn: 4010023.0000 - fn: 171558.0000 - accuracy: 0.9519 - precision: 0.6560 - recall: 0.2760\n",
      "Epoch 2: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 722s 2s/step - loss: 0.1509 - tp: 65404.0000 - fp: 34301.0000 - tn: 4010023.0000 - fn: 171558.0000 - accuracy: 0.9519 - precision: 0.6560 - recall: 0.2760 - val_loss: 0.1617 - val_tp: 9067.0000 - val_fp: 5201.0000 - val_tn: 598957.0000 - val_fn: 25602.0000 - val_accuracy: 0.9518 - val_precision: 0.6355 - val_recall: 0.2615\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1435 - tp: 67461.0000 - fp: 32074.0000 - tn: 4012250.0000 - fn: 169501.0000 - accuracy: 0.9529 - precision: 0.6778 - recall: 0.2847\n",
      "Epoch 3: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 734s 2s/step - loss: 0.1435 - tp: 67461.0000 - fp: 32074.0000 - tn: 4012250.0000 - fn: 169501.0000 - accuracy: 0.9529 - precision: 0.6778 - recall: 0.2847 - val_loss: 0.1625 - val_tp: 8964.0000 - val_fp: 5298.0000 - val_tn: 598860.0000 - val_fn: 25705.0000 - val_accuracy: 0.9515 - val_precision: 0.6285 - val_recall: 0.2586\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1389 - tp: 69501.0000 - fp: 31246.0000 - tn: 4013078.0000 - fn: 167461.0000 - accuracy: 0.9536 - precision: 0.6899 - recall: 0.2933\n",
      "Epoch 4: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 788s 2s/step - loss: 0.1389 - tp: 69501.0000 - fp: 31246.0000 - tn: 4013078.0000 - fn: 167461.0000 - accuracy: 0.9536 - precision: 0.6899 - recall: 0.2933 - val_loss: 0.1639 - val_tp: 8921.0000 - val_fp: 5414.0000 - val_tn: 598744.0000 - val_fn: 25748.0000 - val_accuracy: 0.9512 - val_precision: 0.6223 - val_recall: 0.2573\n",
      "Epoch 5/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1358 - tp: 71235.0000 - fp: 31055.0000 - tn: 4013269.0000 - fn: 165727.0000 - accuracy: 0.9540 - precision: 0.6964 - recall: 0.3006\n",
      "Epoch 5: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 739s 2s/step - loss: 0.1358 - tp: 71235.0000 - fp: 31055.0000 - tn: 4013269.0000 - fn: 165727.0000 - accuracy: 0.9540 - precision: 0.6964 - recall: 0.3006 - val_loss: 0.1654 - val_tp: 8938.0000 - val_fp: 5654.0000 - val_tn: 598504.0000 - val_fn: 25731.0000 - val_accuracy: 0.9509 - val_precision: 0.6125 - val_recall: 0.2578\n",
      "Epoch 6/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1334 - tp: 72761.0000 - fp: 31182.0000 - tn: 4013142.0000 - fn: 164201.0000 - accuracy: 0.9544 - precision: 0.7000 - recall: 0.3071\n",
      "Epoch 6: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 720s 2s/step - loss: 0.1334 - tp: 72761.0000 - fp: 31182.0000 - tn: 4013142.0000 - fn: 164201.0000 - accuracy: 0.9544 - precision: 0.7000 - recall: 0.3071 - val_loss: 0.1671 - val_tp: 8978.0000 - val_fp: 5927.0000 - val_tn: 598231.0000 - val_fn: 25691.0000 - val_accuracy: 0.9505 - val_precision: 0.6023 - val_recall: 0.2590\n",
      "Epoch 7/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1317 - tp: 74098.0000 - fp: 31412.0000 - tn: 4012912.0000 - fn: 162864.0000 - accuracy: 0.9546 - precision: 0.7023 - recall: 0.3127\n",
      "Epoch 7: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 724s 2s/step - loss: 0.1317 - tp: 74098.0000 - fp: 31412.0000 - tn: 4012912.0000 - fn: 162864.0000 - accuracy: 0.9546 - precision: 0.7023 - recall: 0.3127 - val_loss: 0.1688 - val_tp: 8999.0000 - val_fp: 6130.0000 - val_tn: 598028.0000 - val_fn: 25670.0000 - val_accuracy: 0.9502 - val_precision: 0.5948 - val_recall: 0.2596\n",
      "Epoch 8/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1303 - tp: 75343.0000 - fp: 31743.0000 - tn: 4012581.0000 - fn: 161619.0000 - accuracy: 0.9548 - precision: 0.7036 - recall: 0.3180\n",
      "Epoch 8: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 753s 2s/step - loss: 0.1303 - tp: 75343.0000 - fp: 31743.0000 - tn: 4012581.0000 - fn: 161619.0000 - accuracy: 0.9548 - precision: 0.7036 - recall: 0.3180 - val_loss: 0.1704 - val_tp: 9034.0000 - val_fp: 6356.0000 - val_tn: 597802.0000 - val_fn: 25635.0000 - val_accuracy: 0.9499 - val_precision: 0.5870 - val_recall: 0.2606\n",
      "Epoch 9/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1291 - tp: 76475.0000 - fp: 32085.0000 - tn: 4012239.0000 - fn: 160487.0000 - accuracy: 0.9550 - precision: 0.7044 - recall: 0.3227\n",
      "Epoch 9: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 724s 2s/step - loss: 0.1291 - tp: 76475.0000 - fp: 32085.0000 - tn: 4012239.0000 - fn: 160487.0000 - accuracy: 0.9550 - precision: 0.7044 - recall: 0.3227 - val_loss: 0.1719 - val_tp: 9055.0000 - val_fp: 6559.0000 - val_tn: 597599.0000 - val_fn: 25614.0000 - val_accuracy: 0.9496 - val_precision: 0.5799 - val_recall: 0.2612\n",
      "Epoch 10/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1282 - tp: 77497.0000 - fp: 32426.0000 - tn: 4011898.0000 - fn: 159465.0000 - accuracy: 0.9552 - precision: 0.7050 - recall: 0.3270\n",
      "Epoch 10: saving model to checkpoints/in-shop_model_1_STEP2\\checkpoint_VGG_Model_1.ckpt\n",
      "386/386 [==============================] - 737s 2s/step - loss: 0.1282 - tp: 77497.0000 - fp: 32426.0000 - tn: 4011898.0000 - fn: 159465.0000 - accuracy: 0.9552 - precision: 0.7050 - recall: 0.3270 - val_loss: 0.1734 - val_tp: 9074.0000 - val_fp: 6741.0000 - val_tn: 597417.0000 - val_fn: 25595.0000 - val_accuracy: 0.9494 - val_precision: 0.5738 - val_recall: 0.2617\n"
     ]
    }
   ],
   "source": [
    "history_model_1 = model_1.fit(ds_train_batched, \n",
    "                            epochs=10,\n",
    "                            validation_data =ds_test_batched,\n",
    "                            callbacks=[early_stopping, checkpoint_callback]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../trained_models/IN_SHOP_Models/model_1_vgg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../trained_models/IN_SHOP_Models/model_1_vgg\\assets\n"
     ]
    }
   ],
   "source": [
    "model_1.save(\"../trained_models/IN_SHOP_Models/model_1_vgg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_model_1.history).to_csv(\"../trained_models/IN_SHOP_Models/model_1_vgg_RESULT.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Efficient Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "efficient_net.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "inputs = keras.Input(shape=(IMG_WIDTH,IMG_HEIGHT,3)) \n",
    "x = efficient_net(inputs) \n",
    "x = keras.layers.GlobalAveragePooling2D()(x) \n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42) \n",
    "activation = tf.keras.activations.sigmoid  \n",
    "\n",
    "outputs = keras.layers.Dense(nr_of_classes,\n",
    "                             kernel_initializer=initializer, \n",
    "                             activation=activation)(x) \n",
    "\n",
    "model_2 = keras.Model(inputs, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x234804c1640>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.load_weights(\"checkpoints/in-shop_ds_model2/checkpoint_Efficient_Base.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) \n",
    "checkpoint_path = \"checkpoints/in-shop_ds_model2_STEP_2/checkpoint_Efficient_Base.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1496 - tp: 63346.0000 - fp: 28537.0000 - tn: 4015787.0000 - fn: 173616.0000 - accuracy: 0.9528 - precision: 0.6894 - recall: 0.2673\n",
      "Epoch 1: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 291s 721ms/step - loss: 0.1496 - tp: 63346.0000 - fp: 28537.0000 - tn: 4015787.0000 - fn: 173616.0000 - accuracy: 0.9528 - precision: 0.6894 - recall: 0.2673 - val_loss: 0.1440 - val_tp: 9814.0000 - val_fp: 3646.0000 - val_tn: 600512.0000 - val_fn: 24855.0000 - val_accuracy: 0.9554 - val_precision: 0.7291 - val_recall: 0.2831\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1428 - tp: 65136.0000 - fp: 26515.0000 - tn: 4017809.0000 - fn: 171826.0000 - accuracy: 0.9537 - precision: 0.7107 - recall: 0.2749\n",
      "Epoch 2: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 301s 772ms/step - loss: 0.1428 - tp: 65136.0000 - fp: 26515.0000 - tn: 4017809.0000 - fn: 171826.0000 - accuracy: 0.9537 - precision: 0.7107 - recall: 0.2749 - val_loss: 0.1440 - val_tp: 9716.0000 - val_fp: 3616.0000 - val_tn: 600542.0000 - val_fn: 24953.0000 - val_accuracy: 0.9553 - val_precision: 0.7288 - val_recall: 0.2803\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1386 - tp: 66904.0000 - fp: 25868.0000 - tn: 4018456.0000 - fn: 170058.0000 - accuracy: 0.9542 - precision: 0.7212 - recall: 0.2823\n",
      "Epoch 3: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 295s 761ms/step - loss: 0.1386 - tp: 66904.0000 - fp: 25868.0000 - tn: 4018456.0000 - fn: 170058.0000 - accuracy: 0.9542 - precision: 0.7212 - recall: 0.2823 - val_loss: 0.1443 - val_tp: 9729.0000 - val_fp: 3686.0000 - val_tn: 600472.0000 - val_fn: 24940.0000 - val_accuracy: 0.9552 - val_precision: 0.7252 - val_recall: 0.2806\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1355 - tp: 68234.0000 - fp: 25546.0000 - tn: 4018778.0000 - fn: 168728.0000 - accuracy: 0.9546 - precision: 0.7276 - recall: 0.2880\n",
      "Epoch 4: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 296s 762ms/step - loss: 0.1355 - tp: 68234.0000 - fp: 25546.0000 - tn: 4018778.0000 - fn: 168728.0000 - accuracy: 0.9546 - precision: 0.7276 - recall: 0.2880 - val_loss: 0.1449 - val_tp: 9757.0000 - val_fp: 3766.0000 - val_tn: 600392.0000 - val_fn: 24912.0000 - val_accuracy: 0.9551 - val_precision: 0.7215 - val_recall: 0.2814\n",
      "Epoch 5/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1327 - tp: 70023.0000 - fp: 25261.0000 - tn: 4019063.0000 - fn: 166939.0000 - accuracy: 0.9551 - precision: 0.7349 - recall: 0.2955\n",
      "Epoch 5: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 315s 810ms/step - loss: 0.1327 - tp: 70023.0000 - fp: 25261.0000 - tn: 4019063.0000 - fn: 166939.0000 - accuracy: 0.9551 - precision: 0.7349 - recall: 0.2955 - val_loss: 0.1454 - val_tp: 9787.0000 - val_fp: 3826.0000 - val_tn: 600332.0000 - val_fn: 24882.0000 - val_accuracy: 0.9551 - val_precision: 0.7189 - val_recall: 0.2823\n",
      "Epoch 6/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1304 - tp: 71414.0000 - fp: 25239.0000 - tn: 4019085.0000 - fn: 165548.0000 - accuracy: 0.9554 - precision: 0.7389 - recall: 0.3014\n",
      "Epoch 6: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 259s 672ms/step - loss: 0.1304 - tp: 71414.0000 - fp: 25239.0000 - tn: 4019085.0000 - fn: 165548.0000 - accuracy: 0.9554 - precision: 0.7389 - recall: 0.3014 - val_loss: 0.1460 - val_tp: 9883.0000 - val_fp: 3966.0000 - val_tn: 600192.0000 - val_fn: 24786.0000 - val_accuracy: 0.9550 - val_precision: 0.7136 - val_recall: 0.2851\n",
      "Epoch 7/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1282 - tp: 72967.0000 - fp: 25127.0000 - tn: 4019197.0000 - fn: 163995.0000 - accuracy: 0.9558 - precision: 0.7438 - recall: 0.3079\n",
      "Epoch 7: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 255s 660ms/step - loss: 0.1282 - tp: 72967.0000 - fp: 25127.0000 - tn: 4019197.0000 - fn: 163995.0000 - accuracy: 0.9558 - precision: 0.7438 - recall: 0.3079 - val_loss: 0.1465 - val_tp: 9930.0000 - val_fp: 4069.0000 - val_tn: 600089.0000 - val_fn: 24739.0000 - val_accuracy: 0.9549 - val_precision: 0.7093 - val_recall: 0.2864\n",
      "Epoch 8/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1264 - tp: 74105.0000 - fp: 25179.0000 - tn: 4019145.0000 - fn: 162857.0000 - accuracy: 0.9561 - precision: 0.7464 - recall: 0.3127\n",
      "Epoch 8: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 388s 1s/step - loss: 0.1264 - tp: 74105.0000 - fp: 25179.0000 - tn: 4019145.0000 - fn: 162857.0000 - accuracy: 0.9561 - precision: 0.7464 - recall: 0.3127 - val_loss: 0.1470 - val_tp: 9979.0000 - val_fp: 4160.0000 - val_tn: 599998.0000 - val_fn: 24690.0000 - val_accuracy: 0.9548 - val_precision: 0.7058 - val_recall: 0.2878\n",
      "Epoch 9/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1246 - tp: 75614.0000 - fp: 25093.0000 - tn: 4019231.0000 - fn: 161348.0000 - accuracy: 0.9565 - precision: 0.7508 - recall: 0.3191\n",
      "Epoch 9: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 291s 751ms/step - loss: 0.1246 - tp: 75614.0000 - fp: 25093.0000 - tn: 4019231.0000 - fn: 161348.0000 - accuracy: 0.9565 - precision: 0.7508 - recall: 0.3191 - val_loss: 0.1477 - val_tp: 9953.0000 - val_fp: 4175.0000 - val_tn: 599983.0000 - val_fn: 24716.0000 - val_accuracy: 0.9548 - val_precision: 0.7045 - val_recall: 0.2871\n",
      "Epoch 10/10\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.1230 - tp: 76893.0000 - fp: 25053.0000 - tn: 4019271.0000 - fn: 160069.0000 - accuracy: 0.9568 - precision: 0.7543 - recall: 0.3245\n",
      "Epoch 10: saving model to checkpoints/in-shop_ds_model2_STEP_2\\checkpoint_Efficient_Base.ckpt\n",
      "386/386 [==============================] - 265s 686ms/step - loss: 0.1230 - tp: 76893.0000 - fp: 25053.0000 - tn: 4019271.0000 - fn: 160069.0000 - accuracy: 0.9568 - precision: 0.7543 - recall: 0.3245 - val_loss: 0.1481 - val_tp: 10009.0000 - val_fp: 4325.0000 - val_tn: 599833.0000 - val_fn: 24660.0000 - val_accuracy: 0.9546 - val_precision: 0.6983 - val_recall: 0.2887\n"
     ]
    }
   ],
   "source": [
    "history_model_2 = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../trained_models/IN_SHOP_Models/model_2_efficient_net\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../trained_models/IN_SHOP_Models/model_2_efficient_net\\assets\n"
     ]
    }
   ],
   "source": [
    "model_2.save(\"../trained_models/IN_SHOP_Models/model_2_efficient_net\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_model_2.history).to_csv(\"../trained_models/IN_SHOP_Models/model_2_efficient_net.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import clone_model \n",
    "\n",
    "model_2_fine = clone_model(model_2)\n",
    "model_2_fine.set_weights(model_2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True name: input_6\n",
      "True name: efficientnetb0\n",
      "True name: global_average_pooling2d_2\n",
      "True name: dense_2\n"
     ]
    }
   ],
   "source": [
    "for layer in model_2_fine.layers: \n",
    "    print(layer.trainable,\"name:\" ,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efficientnetb0'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef_net_for_fine = model_2_fine.layers[1]\n",
    "model_2_fine.layers[1].name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_net_for_fine.trainable = True  \n",
    "\n",
    "for layer in ef_net_for_fine.layers[:-10]:\n",
    "    layer.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_fine.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True block7a_se_squeeze\n",
      "True block7a_se_reshape\n",
      "True block7a_se_reduce\n",
      "True block7a_se_expand\n",
      "True block7a_se_excite\n",
      "True block7a_project_conv\n",
      "True block7a_project_bn\n",
      "True top_conv\n",
      "True top_bn\n",
      "True top_activation\n"
     ]
    }
   ],
   "source": [
    "for layer in ef_net_for_fine.layers[-10:]: \n",
    "    print(layer.trainable, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2349d965fa0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_fine.load_weights(\"checkpoints/in-shop_ds_model2_fine_tune/checkpoint_Efficient_Base.ckpt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/in-shop_ds_model2_fine_tune_STEP2/checkpoint_Efficient_Base.ckpt\" \n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         save_best_only=False,\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      " 56/386 [===>..........................] - ETA: 3:36 - loss: 0.1231 - tp: 11296.0000 - fp: 3735.0000 - tn: 583397.0000 - fn: 23396.0000 - accuracy: 0.9564 - precision: 0.7515 - recall: 0.3256"
     ]
    }
   ],
   "source": [
    "history_model_2_fine = model_2.fit(ds_train_batched, \n",
    "        validation_data =ds_test_batched,\n",
    "        epochs=20, \n",
    "        initial_epoch=history_model_2.epoch[-1], # start from previous last epoch \n",
    "        callbacks=[early_stopping, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(ef_net_for_fine.layers) / 3)\n",
    "\n",
    "optimizers = [\n",
    "    keras.optimizers.Adam(), # for layer [0] and last layer \n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-6), # ef net [0 to 80] \n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-4), # ef net [80 to 160] \n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-2), # # ef net [160 to :ef net last] \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_and_layers = [\n",
    "    (optimizers[0], model_2_fine.layers[0]), \n",
    "    (optimizers[1], ef_net_for_fine.layers[0:80]), \n",
    "    (optimizers[2], ef_net_for_fine.layers[80:160]),\n",
    "    (optimizers[3], ef_net_for_fine.layers[160:]),\n",
    "    (optimizers[0], model_2_fine.layers[-1]),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_similar-products')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9fdba9d57aea1bb5c6986083e3594030ae55b020a0216061e1820b16640168"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
